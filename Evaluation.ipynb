{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the model\n",
        "\n",
        "Kindly download the model and load on the colab runtime with the same foldername as mentioned in the code. All files should e enclosed in folder."
      ],
      "metadata": {
        "id": "Mn56Hp7sPT_2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zza2xeIYDxxi",
        "outputId": "bcd2a841-2058-4934-a2d5-4d6ab7b24422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UK to US: He is travelling to the theatre.\n",
            "Translated:  He is traveling to the theater.\n"
          ]
        }
      ],
      "source": [
        "# Load the model and tokenizer for future use\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import re\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"my_uk_to_us_t5\").to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"my_uk_to_us_t5\")\n",
        "\n",
        "# Preprocessing function for text\n",
        "def preprocess_text(text):\n",
        "    text = text.strip().lower()  # Remove extra spaces and convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s\\.,!?]', '', text)  # Remove unwanted characters (emoji, special symbols, etc.)\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+([.,!?;])', r'\\1', text)  # Fix spaces before punctuation\n",
        "    text = text.capitalize()  # Capitalize the first letter\n",
        "    return text\n",
        "\n",
        "# Function for Inference\n",
        "def translate_uk_to_us(text):\n",
        "    input_text = \"UK to US: \" + text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "# test_sentence = \"I CoLoUr üé® the centre of my favourite book.\"\n",
        "test_sentence = \"He is travelling to the theatre.\"\n",
        "test_sentence = \"UK to US: \" + preprocess_text(test_sentence)\n",
        "print(test_sentence)\n",
        "print(\"Translated: \",translate_uk_to_us(test_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To incorporate a more structured evaluation of your model, we can compare the predicted translations with the ground-truth US English translations in your test set. By using metrics like BLEU, ROUGE, and accuracy, we can get a more quantitative assessment of the model's performance.\n",
        "\n",
        "Here‚Äôs how you can do it:\n",
        "\n",
        "Steps for Evaluation:\n",
        "BLEU (Bilingual Evaluation Understudy): Measures how many n-grams in the predicted sentence match those in the reference (ground truth).\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation): Measures recall, i.e., how many n-grams in the reference sentence are covered by the prediction.\n",
        "Accuracy: Measures how many predictions match the ground truth exactly.\n",
        "Let‚Äôs modify your code to include these evaluations.\n",
        "\n",
        "## 1. Import Required Libraries\n",
        "First, you need to install and import the required libraries for evaluation:"
      ],
      "metadata": {
        "id": "_wFq9SW6NMxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets nltk rouge_score sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPH-0TfYK5Yf",
        "outputId": "3694be1e-1af1-427e-858b-5c5b29de47d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.1.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu"
      ],
      "metadata": {
        "id": "bfJoLztlK9TK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"input_text,target_text\n",
        "\"I CoLoUr üé® the centre of my favourite book.\",\"I color the center of my favorite book.\"\n",
        "\"He is travelling ‚úàÔ∏è to the THEATRE.\",\"He is traveling to the theater.\"\n",
        "\"I have a flat near the lift.\",\"I have an apartment near the elevator.\"\n",
        "\"I have a flat near the lift. \",\"I have an apartment near the elevator.\"\n",
        "\"The PROGRAMME üóìÔ∏è will start at 6 O'CLOCK.\",\"The program will start at 6 o'clock.\"\n",
        "\"HE has a cheque üí≥ for payment.\",\"He has a check for payment.\"\n",
        "\"She wears jewellery üíé on occasions...\",\"She wears jewelry on occasions.\"\n",
        "\" THEY are Practising   ‚öΩ for the football MATCH.\",\"They are practicing for the soccer game.\"\n",
        "\"He is using a spanner for the repair.\",\"He is using a wrench for the repair.\"\n",
        "\"The aeroplane ‚úàÔ∏è landed on time.\",\"The airplane landed on time.\"\n",
        "\"hello... üòÉ how are you?\",\"hello... üòÉ how are you?\"\n",
        "\"She bought some colour pencils.\",\"She bought some color pencils.\"\n",
        "\"I am going to the lift.\",\"I am going to the elevator.\"\n",
        "\"His behaviour ü§î is unacceptable.\",\"His behavior is unacceptable.\"\n",
        "\"The cheque üí≥ arrived late üò¢.\",\"The check arrived late.\"\n",
        "\"Do you know where the lift is?\",\"Do you know where the elevator is?\"\n",
        "\"The labor union is organizing a programme üóìÔ∏è.\",\"The labour union is organizing a program.\"\n",
        "\"He enjoys playing football ‚öΩ.\",\"He enjoys playing soccer.\"\n",
        "\"I love visiting the theatre.\",\"I love visiting the theater.\"\n",
        "\"Their practise sessions are improving.\",\"Their practice sessions are improving.\"\n",
        "\"He likes the colour red.\",\"He likes the color red.\"\n",
        "\"The cheque has been approved.\",\"The check has been approved.\"\n",
        "\"The aeroplane ‚úàÔ∏è was delayed.\",\"The airplane was delayed.\"\n",
        "\"Their neighbourhood is beautiful.\",\"Their neighborhood is beautiful.\"\n",
        "\"They've cancelled the programme.\",\"They've canceled the program.\"\n",
        "\"She practises yoga regularly.\",\"She practices yoga regularly.\"\n",
        "\"The cheque has not arrived yet.\",\"The check has not arrived yet.\"\n",
        "\"He is organizing a theatre play.\",\"He is organizing a theater play.\"\n",
        "\"I prefer the lift to the stairs.\",\"I prefer the elevator to the stairs.\"\n",
        "\"His behaviour has been exemplary.\",\"His behavior has been exemplary.\"\n",
        "\"Is the cheque ready for collection?\",\"Is the check ready for collection?\"\n",
        "\"Please colour üé® this drawing.\",\"Please color this drawing.\"\n",
        "\"The aeroplane ‚úàÔ∏è has landed safely.\",\"The airplane has landed safely.\"\n",
        "\"They're still practising football ‚öΩ.\",\"They're still practicing soccer.\"\n",
        "\"Her jewellery collection is stunning.\",\"Her jewelry collection is stunning.\"\n",
        "\"What's the programme for tomorrow?\",\"What's the program for tomorrow?\"\n",
        "\"Their labour union is powerful.\",\"Their labor union is powerful.\"\n",
        "\"They enjoy going to the theatre.\",\"They enjoy going to the theater.\"\n",
        "\"Her favourite dish is lasagna.\",\"Her favorite dish is lasagna.\"\n",
        "\"I need to go to the flat.\",\"I need to go to the apartment.\"\n",
        "\"The cheque is invalid.\",\"The check is invalid.\"\n",
        "\"The aeroplane ‚úàÔ∏è is ready for boarding.\",\"The airplane is ready for boarding.\"\n",
        "\"He prefers the colour blue.\",\"He prefers the color blue.\"\n",
        "\"The theatre play was amazing.\",\"The theater play was amazing.\"\n",
        "\"The programme üóìÔ∏è starts at 10 AM.\",\"The program starts at 10 AM.\"\n",
        "\"Their neighbourhood is very welcoming.\",\"Their neighborhood is very welcoming.\"\n",
        "\"Please practise before the event.\",\"Please practice before the event.\"\n",
        "\"Her jewellery is antique.\",\"Her jewelry is antique.\"\n",
        "\"The cheque üí≥ bounced.\",\"The check bounced.\"\n",
        "\"She wears jewellery every day.\",\"She wears jewelry every day.\"\n",
        "\"He works in the theatre.\",\"He works in the theater.\"\n",
        "\"Her behaviour ü§î is strange lately.\",\"Her behavior is strange lately.\"\n",
        "\"The cheque is in processing.\",\"The check is in processing.\"\n",
        "\"They are rehearsing for the programme.\",\"They are rehearsing for the program.\"\n",
        "\"The aeroplane ‚úàÔ∏è is landing shortly.\",\"The airplane is landing shortly.\"\n",
        "\"Her favourite sport is football ‚öΩ.\",\"Her favorite sport is soccer.\"\n",
        "\"The cheque will be sent tomorrow.\",\"The check will be sent tomorrow.\"\n",
        "\"The aeroplane has been delayed again.\",\"The airplane has been delayed again.\"\n",
        "\"They prefer the colour green.\",\"They prefer the color green.\"\n",
        "\"She is visiting the theatre tomorrow.\",\"She is visiting the theater tomorrow.\"\n",
        "\"The programme is about to begin.\",\"The program is about to begin.\"\n",
        "\"The cheque üí≥ is ready for pickup.\",\"The check is ready for pickup.\"\n",
        "\"Her favourite pastime is painting.\",\"Her favorite pastime is painting.\"\n",
        "\"His favourite sport is rugby.\",\"His favorite sport is rugby.\"\n",
        "\"The aeroplane ‚úàÔ∏è is taking off.\",\"The airplane is taking off.\"\n",
        "\"She practises football daily.\",\"She practices soccer daily.\"\n",
        "\"The cheque is overdue.\",\"The check is overdue.\"\n",
        "\"Her behaviour has been concerning.\",\"Her behavior has been concerning.\"\n",
        "\"The cheque is being reissued.\",\"The check is being reissued.\"\n",
        "\"The theatre group is performing tonight.\",\"The theater group is performing tonight.\"\n",
        "\"They are enjoying the programme.\",\"They are enjoying the program.\"\n",
        "\"Their jewellery is made of gold.\",\"Their jewelry is made of gold.\"\n",
        "\"The cheque has been misplaced.\",\"The check has been misplaced.\"\n",
        "\"Her favourite flower is a rose.\",\"Her favorite flower is a rose.\"\n",
        "\"He is practicing football ‚öΩ right now.\",\"He is practicing soccer right now.\"\n",
        "\"Her jewellery box is full.\",\"Her jewelry box is full.\"\n",
        "\"The cheque üí≥ has been canceled.\",\"The check has been canceled.\"\n",
        "\"The aeroplane ‚úàÔ∏è was on time.\",\"The airplane was on time.\"\n",
        "\"He loves the colour yellow.\",\"He loves the color yellow.\"\n",
        "\"She is practising for the marathon.\",\"She is practicing for the marathon.\"\n",
        "\"The programme üóìÔ∏è was postponed.\",\"The program was postponed.\"\n",
        "\"The aeroplane ‚úàÔ∏è has already taken off.\",\"The airplane has already taken off.\"\n",
        "\"The cheque will be delivered tomorrow.\",\"The check will be delivered tomorrow.\"\n",
        "\"They enjoy watching theatre performances.\",\"They enjoy watching theater performances.\"\n",
        "\"She painted the colour blue on the wall.\",\"She painted the color blue on the wall.\"\n",
        "\"He is participating in the programme.\",\"He is participating in the program.\"\n",
        "\"The aeroplane ‚úàÔ∏è was delayed again.\",\"The airplane was delayed again.\"\n",
        "\"The cheque üí≥ is ready for withdrawal.\",\"The check is ready for withdrawal.\"\n",
        "\"She has a collection of beautiful jewellery üíé.\",\"She has a collection of beautiful jewelry.\"\n",
        "\"The cheque is still pending.\",\"The check is still pending.\"\n",
        "\"The aeroplane ‚úàÔ∏è will arrive shortly.\",\"The airplane will arrive shortly.\"\n",
        "\"The theatre's performance was breathtaking.\",\"The theater's performance was breathtaking.\"\n",
        "\"Her behaviour has been commendable.\",\"Her behavior has been commendable.\"\n",
        "\"The cheque was never received.\",\"The check was never received.\"\n",
        "\"The aeroplane ‚úàÔ∏è took off on time.\",\"The airplane took off on time.\"\n",
        "\"She wears jewellery for special occasions.\",\"She wears jewelry for special occasions.\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZuBWxhNOLur6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(StringIO(data))\n",
        "\n",
        "# Apply preprocessing to the input and target text columns\n",
        "df[\"input_text\"] = df[\"input_text\"].apply(preprocess_text)\n",
        "df[\"target_text\"] = df[\"target_text\"].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "1lpJcck0L4YP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define Evaluation Functions\n",
        "We can define functions to calculate BLEU, ROUGE, and accuracy."
      ],
      "metadata": {
        "id": "sO6fGQYTNe4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(predictions, references):\n",
        "    \"\"\"\n",
        "    Compute the BLEU score between predicted and reference texts\n",
        "    \"\"\"\n",
        "    # BLEU uses list of tokenized sentences, where each sentence is a list of words\n",
        "    tokenized_predictions = [pred.split() for pred in predictions]\n",
        "    tokenized_references = [[ref.split()] for ref in references]  # Reference is a list of lists\n",
        "\n",
        "    return corpus_bleu(tokenized_references, tokenized_predictions)\n",
        "\n",
        "def compute_rouge(predictions, references):\n",
        "    \"\"\"\n",
        "    Compute the ROUGE score between predicted and reference texts\n",
        "    \"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
        "\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        score = scorer.score(ref, pred)\n",
        "        rouge_scores[\"rouge1\"].append(score[\"rouge1\"].fmeasure)\n",
        "        rouge_scores[\"rouge2\"].append(score[\"rouge2\"].fmeasure)\n",
        "        rouge_scores[\"rougeL\"].append(score[\"rougeL\"].fmeasure)\n",
        "\n",
        "    # Average the scores\n",
        "    rouge_scores = {k: sum(v) / len(v) for k, v in rouge_scores.items()}\n",
        "    return rouge_scores\n",
        "\n",
        "def compute_accuracy(predictions, references):\n",
        "    \"\"\"\n",
        "    Compute the exact match accuracy\n",
        "    \"\"\"\n",
        "    exact_matches = sum([1 for pred, ref in zip(predictions, references) if pred == ref])\n",
        "    return exact_matches / len(predictions)\n",
        "\n",
        "def compute_sacrebleu(predictions, references):\n",
        "    \"\"\"\n",
        "    Compute the SacreBLEU score\n",
        "    \"\"\"\n",
        "    return sacrebleu.corpus_bleu(predictions, [references])\n",
        "\n"
      ],
      "metadata": {
        "id": "yJoym7CbK95w"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU Score Calculation\n",
        "The BLEU score measures how many n-grams in the prediction match with the reference text.\n",
        "\n",
        "ROUGE Score Calculation\n",
        "The ROUGE score compares the overlap of n-grams, recall-based measures, between the predicted and reference text.\n",
        "\n",
        "Accuracy Calculation\n",
        "Accuracy measures how many exact matches there are between predicted and reference sentences.\n",
        "\n",
        "SacreBLEU Calculation\n",
        "SacreBLEU is an implementation of BLEU that standardizes the evaluation and uses a common tokenizer, often preferred for consistent BLEU scoring."
      ],
      "metadata": {
        "id": "RAKBmk_TNi3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Evaluate the Model\n",
        "Now, we can integrate these evaluation metrics into the evaluation part of your code. For each sentence in the test set, we will predict the translation using your fine-tuned model and then calculate the metrics."
      ],
      "metadata": {
        "id": "8z5-ZWSXNxxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare lists to store predictions and references\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "# Loop through the test dataset to generate predictions and store references\n",
        "for text, target_text in zip(df[\"input_text\"], df[\"target_text\"]):\n",
        "    text = \"UK to US: \" + preprocess_text(text)\n",
        "\n",
        "    # Generate prediction\n",
        "    pred = translate_uk_to_us(text)\n",
        "    predictions.append(pred)\n",
        "    references.append(target_text)\n",
        "\n",
        "# Calculate BLEU\n",
        "bleu_score = compute_bleu(predictions, references)\n",
        "print(f\"BLEU score: {bleu_score}\")\n",
        "\n",
        "# Calculate ROUGE\n",
        "rouge_scores = compute_rouge(predictions, references)\n",
        "print(f\"ROUGE scores: {rouge_scores}\")\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = compute_accuracy(predictions, references)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Calculate SacreBLEU\n",
        "sacrebleu_score = compute_sacrebleu(predictions, references)\n",
        "print(f\"SacreBLEU score: {sacrebleu_score.score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLSyPTSeK-Di",
        "outputId": "0e3db435-8499-41b1-dea5-b5cb66499768"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0.9598071478068125\n",
            "ROUGE scores: {'rouge1': 0.9796626984126983, 'rouge2': 0.9754464285714285, 'rougeL': 0.9796626984126983}\n",
            "Accuracy: 92.71%\n",
            "SacreBLEU score: 96.94409002653215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Result Interpretation\n",
        "- BLEU Score: Typically ranges from 0 to 1, where a higher score means that the generated translations are closer to the reference translations in terms of n-gram overlap.\n",
        "- ROUGE Scores: These will give you insight into how much overlap exists in terms of recall (how many n-grams from the reference are in the prediction).\n",
        "- Accuracy: The percentage of exact matches between the predicted and reference translations.\n",
        "- SacreBLEU Score: Similar to BLEU but more consistent and reliable due to standardized tokenization."
      ],
      "metadata": {
        "id": "5EjZwoWRN4_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrics Breakdown:\n",
        "#### BLEU Score: 0.96\n",
        "\n",
        "This is an excellent BLEU score, indicating that the model's predictions match the reference translations quite well in terms of n-gram overlap. A score above 0.9 generally suggests high translation quality.\n",
        "\n",
        "#### ROUGE Scores:\n",
        "\n",
        "ROUGE-1: 0.98 ‚Äì The model has a very high recall of unigrams (individual words), meaning it's capturing a lot of the key words in the reference.\n",
        "\n",
        "ROUGE-2: 0.98 ‚Äì A high recall of bigrams (two-word sequences), suggesting the model is capturing key two-word combinations well.\n",
        "\n",
        "ROUGE-L: 0.98 ‚Äì This is a recall-based metric that focuses on the longest matching subsequence of words, and this score suggests that the model is capturing the flow and structure of the sentences well.\n",
        "\n",
        "#### Accuracy: 92.71%\n",
        "\n",
        "This indicates that nearly 93% of the predicted sentences exactly match the reference sentences, which is a strong performance in terms of exact matches.\n",
        "\n",
        "#### SacreBLEU Score: 96.94\n",
        "\n",
        "This score is extremely high and reinforces that the model's predictions are very close to the reference translations when considering standardized tokenization.\n",
        "\n",
        "#### Conclusion:\n",
        "These evaluation results indicate that your model is performing exceptionally well in converting UK English to US English, both in terms of lexical accuracy (correct word choices) and structural accuracy (sentence flow). The SacreBLEU score in particular highlights that the translations are consistent and robust when using a standardized evaluation framework."
      ],
      "metadata": {
        "id": "UcKUcNzWOa8K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAtjqd4vK-Jy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcBBLMFSK-P9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SGtyDepEK-Vu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LVBQA-30K-bV"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}